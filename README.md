# NF-FRAGMENTOMICS PIPELINE

<img src="assets/img/logo.png" alt="logo" width="200">


## Introduction

Recent research indicates that analyzing the fragmentation patterns of circulating free DNA (cfDNA) from genome sequencing data can provide insights into nucleosome occupancy in the original cells. In accessible genomic regions, nucleosomes are arranged systematically to facilitate access for DNA-binding proteins.

Here we present **nf-fragmentomics**, a bioinformatics pipeline designed to automate and standardize the analysis of cfDNA fragmentation patterns. The pipeline takes as input BAM files (aligned sequencing reads), applies fragment size selection, corrects for GC bias, and generates normalized coverage profiles over user-defined genomic regions (such as transcription factor binding sites). It leverages established tools like deepTools for matrix computation and visualization, and supports the use of blacklists to exclude problematic genomic regions. The modular design allows for flexible configuration and integration into various analysis workflows, making it suitable for both exploratory and large-scale studies of cfDNA fragmentation.

<img src="assets/img/cfDNA_degradation.png" alt="cfDNA degradation" width="480">

## Pipeline Summary

This pipeline calculates the composite coverage over specific genome regions.

<img src="assets/img/nf-fragmentomics_base.png" alt="pipeline schema" width="480">

BAM input files are filtered by size (cfDNA fragments have sizes between 90 and 150 bp) and corrected for GC-bias using the method proposed by [Benjamini & Speed (2012). Nucleic Acids Research, 40(10)].

The input BAM is then converted into a coverage file (bigWiggle). At this stage, it is possible to use a blacklist of genomic regions. We are blacklisting the Problematic Regions of the Genome defined by ENCODE. [Amemiya, H.M., Kundaje, A. & Boyle, A.P. The ENCODE Blacklist: Identification of Problematic Regions of the Genome. Sci Rep 9, 9354 (2019). https://doi.org/10.1038/s41598-019-45839-z]

With [computeMatrix](https://deeptools.readthedocs.io/en/latest/content/tools/computeMatrix.html), we calculate the coverage of the signal (BAM file) on a set of targets (BED file). In our analysis, this is typically a set of Transcription Factor Binding Sites regions. We are using the `reference-point` mode: we compute the signal distribution relative to a point (the TFBS center point in our case), expanding on both sides for **4kb**.

A first plot of the composite coverage on the TFBS target set (raw coverage) is generated by the process [plotHeatmap](https://deeptools.readthedocs.io/en/latest/content/tools/plotHeatmap.html).

<img src="assets/img/plotHeatmap_example.png" alt="plotHeatmap example" width="80">

The process `PEAK_STATS` generates 4 output files:

 - `matrix.RDS` matrix in RDS format
 - `peak_data.tsv` file contains the composite coverage (`coverage`) for each bin (`bin`), the coverage relative to the background median (`relative`), and the background mean (`background_mean`).
 - `peak_stats.tsv` summary statistics (see below)
 - `RawSignal.pdf` pdf plot of the calculated metrics
 - `RelativeSignal.pdf` pdf plot for the relative signal metrics


### Peak metrics

<img src="assets/img/peakStats.png" alt="plotStats plot" width="1024">

1. `signal`: name of input sample
2. `target`: name of input target set
3. `source`: source of target
4. `integration`: montecarlo integration of peak using `background.mean`
5. `background.mean`: mean calculated of extreme of distribution.
6. `referencePoint.bin`: bin location of referencePoint.
7. `referencePoint.coverage`: composite coverage at reference point.
8. `referencePoint.relative`: composite relative coverage at reference point.
9. `central.coverage`: composite central coverage as defined in [Griffin](https://www.nature.com/articles/s41467-022-35076-w).
10. `central.coverage.bin.min`: bin left limit of composite central coverage.
11. `central.coverage.bin.max`: bin right limit of composite central coverage.
12. `background.left.limit`: bin left limit for `background.mean`.
13. `background.right.limit`: bin right limit for `background.mean`.
14. `average.coverage`: average coverage as defined in [Griffin](https://www.nature.com/articles/s41467-022-35076-w).
15. `average.coverage.bin.min`: bin left limit of composite average coverage.
16. `average.coverage.bin.max`: bin right limit of composite average coverage.
17. `peak.length`: length of peak (green dotted line in the example)
18. `peak.relative.length`: length of peak (green dotted line in the example)


## Quick Start

```mermaid
flowchart TB
    subgraph " "
    subgraph params
    v4["input"]
    v2["blacklist_bed"]
    v0["genome_2bit"]
    v6["targets"]
    end
    v10([FRAGMENTOMICS])
    v11([VERSIONS])
    v0 --> v10
    v2 --> v10
    v4 --> v10
    v6 --> v10
    v10 --> v11
    end
```
Required input params:

```
input: "./examples/input/example_samplesheet_bam.csv"
targets: "./examples/input/example_targets.csv"
outdir: "./results"
genome_2bit: "./tests/input/stub/GRCh38.2bit"
blacklist_bed: "./tests/input/stub/ENCODE_Blacklist_V2.bed"
```

Other parameters

```
// preprocess: if true, preprocess the input data to bw
preprocess = true
// bin_size: bin size for the bw files
bin_size = 10
// target_expand_sx: bp to expand on the left of the target
target_expand_sx = 4000
// target_expand_dx: bp to expand on the right of the target
target_expand_dx = 4000
// filter_min: minimum fragment length
filter_min = 90
// filter_max: maximum fragment length
filter_max = 150
// collate size for the target channel
collate_size = 25
```

With `collate_size` you can control the granularity of target processing (the number of targets for process), default is 15.


### INPUT

Input is a samplesheet with bam or bw (bigWiggle) samples.

Example of `samplesheet.csv`:

```
caseid,sampleid,timepoint,bam,bai,bw
PATIENT_A,PATIENT_A_T1,T1,/path/to/bam.bam,/path/to/bam.bai,
PATIENT_A,PATIENT_A_T2,T2,/path/to/bam.bam,/path/to/bam.bai,
```

Where:

 - `caseid` is the patient
 - `sampleid` is the sample
 - `timepoint` is the time group
 - `bam` is the input BAM file
 - `bai` is the BAM index
 - `bw` is the (optional) big wiggle file (preprocessing will be skipped)

### TARGETS

Example of target list (`targets.csv`):

```
name,source,bed
MYC,GRIFFIN,./tests/input/stub/myc.bed
ELK4,TIMON,./tests/input/stub/elk4.bed
rand1,house_keeping_dataset,./tests/input/stub/rand1.bed
rand2,house_keeping_dataset,./tests/input/stub/rand2.bed
rand3,house_keeping_dataset,./tests/input/stub/rand3.bed
HouseKeeping,house_keeping_dataset,./tests/input/stub/GeneHancer_housekeeping.bed
```

Where:

 - `name`: name of the target
 - `source`: source of the target is, in many cases, the enclosing folder of the bed file. Different sources will be separated in publish dir. This allows the presence of files with the same name but ib different folders (griffin/MYC.bed and jasper/MYC.bed)
 - `bed`: bed file with targets


### GENOME

`genome_2bit`: Genome in two-bit format. Most genomes can be found here: http://hgdownload.cse.ucsc.edu/gbdb/

### BLACKLIST BED

`blacklist_bed`: BED file with blacklisted regions used in COVERAGEBAM (wiggle file generation) and in COMPUTEMATRIX (matrix calculation).

We are using the ENCODE blacklist from:

    Amemiya, H.M., Kundaje, A. & Boyle, A.P. The ENCODE Blacklist: Identification of Problematic Regions of the Genome. Sci Rep 9, 9354 (2019). https://doi.org/10.1038/s41598-019-45839-z

### GENOME SIZE

`genome_size`: effective genome size used by GC Correction functions (see also: [deeptools effective genome size page](https://deeptools.readthedocs.io/en/latest/content/feature/effectiveGenomeSize.html))


### PROFILES

 - `stub`: stub run files
 - `debug`: run in debug mode
 - `devel`: run locally
 - `test`: test input files
 - `test_full`: full test input files
 - `conda`: conda container mode
 - `docker`: docker container mode
 - `singularity`: singularity container mode
 - `hpc`: for our HPC
 - `arm`: for docker on osx


Some examples:

Local stub run with stub annotation files:

```
conda activate nf-fragmentomics-env
nextflow run main.nf -params-file examples/input/example_params.yaml -profile stub -stub-run
```

Run on single machine with conda environment:

```
nextflow run main.nf -profile devel,conda -params-file params.yaml
```

Run on a cluster with singularity

```
nextflow run main.nf -profile hpc,singularity -params-file params.yaml
```

## Documentation

### Samplesheet specifications

We tried the analysis with WGS and lpWGS samples.

Input BAM file must be sorted and indexed.

### Targets specifications

File name or names, in BED or GTF format, containing the regions to compute and plot.

see also [computeMatrix](https://deeptools.readthedocs.io/en/latest/content/tools/computeMatrix.html)

### Required Annotation files

This pipeline use the ENCODE blacklist or other blacklist BED file to remove problematic regions from the analysis.

Param: `blacklist_bed`

### Parameters

 - `preprocess`: if true, the pipeline filters BAM by size, applies GC correction, and converts to wiggle file
 - `bin_size`: the bin size used to generate the coverage file (big wiggle)
 - `target_expand_sx` and `target_expand_dx`: how many bp to expand the Target region? default is 4000 bp on both sides
 - `filter_min` and `filter_max`: limit for reads filtering. By default is 90-150 bp

### Labels

Configured labels:

 - process_low
 - process_medium
 - process_high

`process_low`: BAMPEFRAGMENTSIZE, PLOTCOVERAGE
`process_medium`: HEATMAP, PEAK_STATS
`process_high`: COMPUTEGCBIAS, CORRECTGCBIAS, COMPUTEMATRIX, COVERAGEBAM, FILTERBAMBYSIZE


### Scripts

Utility scripts in the `scripts` directory.

#### SampleSheet Generator

location: `scripts/samplesheet_generator.py`

```
usage: samplesheet_generator.py [-h] [-r REGEXP] [-v] [-vv] FILE [FILE ...]

Generate samplesheet.csv for nf-fragmentomics pipeline

positional arguments:
  FILE                  BAM or wiggle files

options:
  -h, --help            show this help message and exit
  -r REGEXP, --regexp REGEXP
                        Parser regexp - default: ((.*)_(.*))\..*
  -v, --verbose         set loglevel to INFO
  -vv, --very-verbose   set loglevel to DEBUG

Author: Davide Rambaldi
```

Usage example with test files:

```
./scripts/samplesheet_generator.py tests/input/samplesheet_generator/*/*.{bw,bam}
```

Example with custom regexp

```
./scripts/samplesheet_generator.py /Volumes/sottoriva/00-PROCESSED_DATA/2022-MAYA_MISSONI_VALENTINO/CASES/MAYA_*/MAYA_*/low_pass_wgs/bwa/MAYA_*_filter_90_150_processed.bam -r "((.*)_(POST_TMZ_INDUCTION|BL|ON_ICI|PD))_filter_90_150_processed\..*"
```

#### Targets Generator

location: `scripts/targets_generator.py`

```
usage: targets_generator.py [-h] [-v] [-vv] FILE [FILE ...]

Generate targets.csv for nf-fragmentomics pipeline

positional arguments:
  FILE                  BED files

options:
  -h, --help            show this help message and exit
  -v, --verbose         set loglevel to INFO
  -vv, --very-verbose   set loglevel to DEBUG

Author: Davide Rambaldi
```

Usage example with test files:

```
./scripts/targets_generator.py tests/input/targets_generator/*.bed
```

## Analysis example

<!-- TODO -->

## Credits

<!-- TODO -->

## Contributions and Support

<!-- TODO -->

## Citation

<!-- TODO -->